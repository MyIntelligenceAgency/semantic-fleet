{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Multi-Connector Optimization with Explicit Summarization Prompts\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook, we aim to demonstrate the power of multi-connector optimization. Specifically, we'll use a straightforward summarization prompt to show how the system can intelligently choose between different connectors (ChatGPT and Oobabooga models) based on performance metrics like speed and cost.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **OpenAI Key**: Make sure you've run `00-AI-settings.ipynb` to set up your OpenAI key. We'll use ChatGPT as our primary connector.\n",
    "- **Oobabooga Model**: Make sure you have at least one Oobabooga model up and running. This model should be configured in the `Multiconnector` section of your `settings.json` file. The number of models you can run and their size will depend on your VRam capabilities. Suggestions are made in the multistart scripts and default settings.\n",
    "- **Prior Knowledge**: A basic understanding of the multi-connector pipeline is required. If you're new to this, consider going through `03-multiConnector-intro-with-arithmetic-mocks.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Import Required Libraries\n",
    "\n",
    "First things first, let's import all the necessary libraries. These libraries will help us in loading configurations, running the multi-connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.Extensions.Configuration, 7.0.0</span></li><li><span>Microsoft.Extensions.Configuration.Binder, 7.0.4</span></li><li><span>Microsoft.Extensions.Configuration.Json, 7.0.0</span></li><li><span>MyIA.SemanticKernel.Connectors.AI.MultiConnector, 0.34.3</span></li><li><span>MyIA.SemanticKernel.Connectors.AI.Oobabooga, 0.34.3</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Import package for loading hierarchichal settings from settings.json\n",
    "#r \"nuget: Microsoft.Extensions.Configuration\"\n",
    "#r \"nuget: Microsoft.Extensions.Configuration.Json\"\n",
    "#r \"nuget: Microsoft.Extensions.Configuration.Binder\"\n",
    "\n",
    "// Import Oobabooga connector package\n",
    "#r \"nuget: MyIA.SemanticKernel.Connectors.AI.Oobabooga\"\n",
    "// Import Multiconnector package\n",
    "#r \"nuget: MyIA.SemanticKernel.Connectors.AI.MultiConnector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import semanti kernel package too. This is pretty much optional since Multiconnector package contains a reference to it. But it will tell us which version is in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>microsoft.semantickernel, 1.0.0-beta6</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Import Semantic Kernel\n",
    "#r \"nuget: Microsoft.SemanticKernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Settings\n",
    "\n",
    "Here, we load the OpenAI and Multiconnector configurations from the settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "// Load configuration using builder package\n",
    "using System.IO;\n",
    "using Microsoft.Extensions.Configuration;\n",
    "using MyIA.SemanticKernel.Connectors.AI.MultiConnector.Configuration;\n",
    "\n",
    "var builder = new ConfigurationBuilder()\n",
    "    .SetBasePath(Directory.GetCurrentDirectory())\n",
    "    .AddJsonFile(\"config/settings.json\", optional: false, reloadOnChange: true);\n",
    "\n",
    "IConfiguration configuration = builder.Build();\n",
    "\n",
    "var openAIConfiguration = configuration.GetSection(\"OpenAI\").Get<OpenAIConfiguration>();\n",
    "var multiOobaboogaConnectorConfiguration = configuration.GetSection(\"MultiConnector\").Get<MultiOobaboogaConnectorConfiguration>();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set up MultiTextCompletion settings\n",
    "\n",
    "There are many parameters controlling how the multiconnector will work and perform optimization. We need to create an instance of the corresponding class.\n",
    "\n",
    "Also, because we'll be measuring costs to perform our optimization, we need to create an creditor object dedicated to that, and we'll configure the settings to only account for completion request costs, discarding concern about duration. \n",
    "\n",
    "We'll stick to default parameters for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\r\n",
       "  \"FreezePromptTypes\": false,\r\n",
       "  \"PromptTruncationLength\": 20,\r\n",
       "  \"AdjustPromptStarts\": false,\r\n",
       "  \"EnablePromptSampling\": true,\r\n",
       "  \"MaxInstanceNb\": 10,\r\n",
       "  \"AnalysisSettings\": {\r\n",
       "    \"EnableAnalysis\": false,\r\n",
       "    \"AnalysisFilePath\": \".\\\\MultiTextCompletion-analysis.json\",\r\n",
       "    \"AnalysisDelay\": \"00:00:01\",\r\n",
       "    \"AnalysisAwaitsManualTrigger\": false,\r\n",
       "    \"EnableConnectorTests\": true,\r\n",
       "    \"TestPrimaryCompletion\": true,\r\n",
       "    \"TestsPeriod\": \"00:00:10\",\r\n",
       "    \"MaxDegreeOfParallelismTests\": 1,\r\n",
       "    \"MaxDegreeOfParallelismConnectorsByTest\": 3,\r\n",
       "    \"EnableTestEvaluations\": true,\r\n",
       "    \"EvaluationPeriod\": \"00:00:10\",\r\n",
       "    \"MaxDegreeOfParallelismEvaluations\": 5,\r\n",
       "    \"UseSelfVetting\": false,\r\n",
       "    \"EnableSuggestion\": true,\r\n",
       "    \"SuggestionPeriod\": \"00:01:00\",\r\n",
       "    \"UpdateSuggestedSettings\": true,\r\n",
       "    \"SaveSuggestedSettings\": false,\r\n",
       "    \"DeleteAnalysisFile\": true,\r\n",
       "    \"MultiCompletionSettingsFilePath\": \".\\\\MultiTextCompletionSettings.json\",\r\n",
       "    \"NbPromptTests\": 1,\r\n",
       "    \"VettingPromptTransform\": {\r\n",
       "      \"Template\": \"Validate a text completion model\\u0027s response to a semantic function: following are a templated prompt sent to a large language model and the completion it returned, to be evaluated. Please indicate whether the response is valid or not.\\n{SemanticRemarks}\\nDoes the response appropriately completes the prompt? Please answer simply with true or false.\\nPROMPT:\\n-------\\n{prompt}\\nRESPONSE:\\n---------\\n{0}\\nRESPONSE IS VALID? (true/false):\\n--------------------------------\\n\",\r\n",
       "      \"InterpolationType\": 1\r\n",
       "    },\r\n",
       "    \"VettingRequestSettings\": {\r\n",
       "      \"service_id\": null,\r\n",
       "      \"model_id\": null,\r\n",
       "      \"MAX_TOKENS\": 1,\r\n",
       "      \"TEMPERATURE\": 0,\r\n",
       "      \"RESULTSPERPROMPT\": 1\r\n",
       "    }\r\n",
       "  },\r\n",
       "  \"LogCallResult\": false,\r\n",
       "  \"LogTestCollection\": false,\r\n",
       "  \"PromptLogsJsonEncoded\": true,\r\n",
       "  \"PromptLogTruncationLength\": 2000,\r\n",
       "  \"PromptLogTruncationFormat\": \"{0}  (...)   {1}\",\r\n",
       "  \"PromptMultiConnectorSettings\": [],\r\n",
       "  \"GlobalParameters\": {\r\n",
       "    \"SystemSupplement\": \"Assume you\\u0027re about to engage in the \\u0027semantic-function game\\u0027. In this context, every incoming prompt will be based on a semantic function, even if it\\u0027s not perfectly formed or seems ambiguous. Your primary goal is to identify and execute the core function or intent of the message, filtering out noise or extraneous details. Treat the following prompt as a function and provide a direct, precise completion without added commentary. Prioritize the most likely and salient function based on the information presented. Be alert to cues, even if they\\u0027re subtle or embedded, and strive to respond as accurately and succinctly as possible.\",\r\n",
       "    \"UserPreamble\": \"Let\\u0027s engage in a game: carefully heed the upcoming directives. Respond solely with a continuation of my message, abstaining from any extra remarks.\",\r\n",
       "    \"SemanticRemarks\": \"Semantic functions are structured templates that work in tandem with other similar templates and native code-based functions. The native functions, in particular, demand a precise adherence to given instructions. They rely heavily on accurately parsed input parameters and lack mechanisms to sift through any unnecessary noise.\\nWhen assessing the appropriateness of a response, it\\u0027s crucial to be discerning. While instructions often present an example that outlines the desired response format, these examples may not always be exhaustive. Occasionally, they might even be overly simplistic, intended merely to keep the instructions concise. Thus, always ensure that you thoroughly understand and thoughtfully apply these instructions to generate a fitting answer to the given input.\"\r\n",
       "  },\r\n",
       "  \"GlobalPromptTransform\": null,\r\n",
       "  \"Creditor\": {\r\n",
       "    \"OngoingCost\": 0\r\n",
       "  },\r\n",
       "  \"SampleCollectionDelay\": \"00:00:00.0200000\",\r\n",
       "  \"SampleVettedConnectors\": true\r\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MyIA.SemanticKernel.Connectors.AI.MultiConnector;\n",
    "using System.Text.Json;\n",
    "using System.Text.Json.Serialization;\n",
    "using MyIA.SemanticKernel.Connectors.AI.MultiConnector.PromptSettings;\n",
    "\n",
    "var creditor = new CallRequestCostCreditor();\n",
    "\n",
    "// The most common settings for a MultiTextCompletion are illustrated below, most of them have default values and are optional\n",
    "var settings = new MultiTextCompletionSettings()\n",
    "{\n",
    "    Creditor = creditor,\n",
    "    // We set connectors comparer to only attend to completion cost\n",
    "    ConnectorComparer = MultiTextCompletionSettings.GetWeightedConnectorComparer(0,1),\n",
    "    AnalysisSettings = new()\n",
    "    {\n",
    "        // We set the maximum number of tests to perform to 1.\n",
    "        // Alternatively, we could define a temperature transformation to set a positive temperature and gather distinct test results from our single prompt\n",
    "        NbPromptTests = 1\n",
    "    } \n",
    "};\n",
    "\n",
    "string jsonString = JsonSerializer.Serialize(settings, new JsonSerializerOptions() { WriteIndented = true });\n",
    "display(jsonString);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialization\n",
    "\n",
    "With all the settings created, we can now create the semantic kernel that we'll use to run our tests.\n",
    "\n",
    "### 2.1 Create primary and secondary completions\n",
    "\n",
    "In this step, we'll initialize our primary and secondary text completions. The primary completion will be based on the OpenAI configuration, while the secondary ones will be based on the Oobabooga models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name of Primary Completion: gpt-3.5-turbo"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name of Secondary Completion #0: TheBloke_Synthia-13B-v1.2-GGUF"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name of Secondary Completion #1: TheBloke_Mistral-7B-OpenOrca-GGUF"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using System.Threading;\n",
    "using Microsoft.SemanticKernel.AI.TextCompletion;\n",
    "using Microsoft.SemanticKernel.Connectors.AI.OpenAI.ChatCompletion;\n",
    "using Microsoft.SemanticKernel.Connectors.AI.OpenAI.TextCompletion;\n",
    "\n",
    "// Creating a cancellation token source to be able to cancel the request\n",
    "CancellationTokenSource cleanupToken = new();\n",
    "\n",
    "//Creating the primary connector. We use the OpenAI connector here, either text or chat completion depending on the configuration\n",
    " ITextCompletion openAiConnector;\n",
    "\n",
    " string testOrChatModelId;\n",
    " if (openAIConfiguration.ChatModelId != null)\n",
    " {\n",
    "     testOrChatModelId = openAIConfiguration.ChatModelId;\n",
    "     openAiConnector = new OpenAIChatCompletion(testOrChatModelId, openAIConfiguration.ApiKey);\n",
    " }\n",
    " else\n",
    " {\n",
    "     testOrChatModelId = openAIConfiguration.ModelId;\n",
    "     openAiConnector = new OpenAITextCompletion(testOrChatModelId, openAIConfiguration.ApiKey);\n",
    " }\n",
    "\n",
    " // Creating the corresponding named completion\n",
    " var openAiNamedCompletion = new NamedTextCompletion(testOrChatModelId, openAiConnector)\n",
    "{\n",
    "    MaxTokens = openAIConfiguration.MaxTokens,\n",
    "    CostPer1000Token = openAIConfiguration.CostPer1000Token,\n",
    "    TokenCountFunc = MultiOobaboogaConnectorConfiguration.TokenCountFunctionMap[openAIConfiguration.TokenCountFunction],\n",
    "    //We did not observe any limit on Open AI concurrent calls\n",
    "    MaxDegreeOfParallelism = 5,\n",
    "};\n",
    "\n",
    "display($\"Name of Primary Completion: {openAiNamedCompletion.Name}\");\n",
    "\n",
    " // Creating the secondary connectors. We use a dedicated helper, but you can create them manually if you want.\n",
    " var oobaboogaCompletions = multiOobaboogaConnectorConfiguration.CreateNamedCompletions();\n",
    "\n",
    " for(int i = 0; i < oobaboogaCompletions.Count; i++)\n",
    " {\n",
    "        display($\"Name of Secondary Completion #{i}: {oobaboogaCompletions[i].Name}\");\n",
    " }\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Kernel\n",
    "\n",
    "Now that we have our primary and secondary completions, we can create a semantic kernel and instantiate our multiconnector completion from settings, primary and secondary completions.\n",
    "\n",
    "We use the dedicated helper for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "var builder = Microsoft.SemanticKernel.Kernel.Builder;\n",
    "\n",
    "builder.WithMultiConnectorCompletionService(\n",
    "    serviceId: null,\n",
    "    settings: settings,\n",
    "    mainTextCompletion: openAiNamedCompletion,\n",
    "    setAsDefault: true,\n",
    "    analysisTaskCancellationToken: cleanupToken.Token,\n",
    "    otherCompletions: oobaboogaCompletions.ToArray());\n",
    "\n",
    "var kernel = builder.Build();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create simple inline semantic function\n",
    "\n",
    "We'll create a simple inline semantic function that takes a long text and summarizes it. This function will serve as our test case for multi-connector optimization.\n",
    "\n",
    "In the next notebook, we'll move on with considering skills, inputs of various complexities, static and finally dynamic plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel;\n",
    "using MyIA.SemanticKernel.Connectors.AI.MultiConnector;\n",
    "\n",
    "var text = @\"A long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\n",
    "\n",
    "After that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\n",
    "\n",
    "Next, there was a telephone. With it, people could talk and listen to friends who were far. They didn’t have to wait for letters anymore.\n",
    "\n",
    "Then, there was a thing called television. People could watch stories on it, like a play. They didn’t need to go outside.\n",
    "\n",
    "Lastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\";\n",
    "\n",
    "var prompt = $\"Summarize the following text in one sentence:\\n{text}\\n\\nSummary:\";\n",
    "\n",
    "var simpleSemanticFunction = kernel.CreateSemanticFunction(prompt, requestSettings: new MultiCompletionRequestSettings(){MaxTokensMulti = 100} );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running and optimizing settings\n",
    "\n",
    "Now that everything is in order we'll follow the following workflow:\n",
    "\n",
    "- The plan is run once. The primary connector defined (Chat GPT) is used to generate our completion.\n",
    "    - Performance in cost and in duration is recorded.\n",
    "    - Samples are collected automatically during the run\n",
    "    - Result of the plan is shown.\n",
    "- An analysis task is run from samples collected during the run.\n",
    "    - Each connector is tested on the samples.\n",
    "    - The primary connector (ChatGPT) evaluates the test runs, vetting each connector's capability to handle each corresponding prompt type.\n",
    "    - New settings are computed from the evaluation. Vetted connectors are promoted to handle the corresponding prompt types.\n",
    "    - MultiCompletion settings are updated according to the analysis results.\n",
    "- The original plan is reloaded and run again. This time, the secondary connectors may be used to generate some or all of the completions according to the updated settings.\n",
    "    - Performance in cost and in duration is recorded.\n",
    "    - Result of the plan is shown\n",
    "\n",
    "### 3.1 Run function with Primary Connector\n",
    "\n",
    "For this first example, we want our multiconnector to do the job automatically for us, so we'll configure our settings accordingly before we run our function.\n",
    "\n",
    "We'll simply register to the final optimization event in order to figure out when our multiconnector has finished vetting our secondary connectors.\n",
    "\n",
    "Note that because the optimization task in only triggered with new prompts, events won't be triggered and code won't terminate if you run it twice. If you wish to rerun optimization, you should rerun the code block that creates the settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result from primary connector: The text describes the evolution of communication methods from handwritten letters to the printing press, telephone, television, and finally mobile phones and computers with internet and social media capabilities."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Cost from running primary connector's completion: 0,000324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluation for secondary connectors finished"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Optimization task finished"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MyIA.SemanticKernel.Connectors.AI.MultiConnector.Analysis;\n",
    "\n",
    "// We enable promlpt sampling and analysis so that the multiconnector tests our prompt on our secondary connectors after it is run on the primary connector\n",
    "settings.EnablePromptSampling = true;\n",
    "settings.AnalysisSettings.EnableAnalysis = true;\n",
    "\n",
    "// Subscribe to the Evaluation completed event\n",
    "TaskCompletionSource<EvaluationCompletedEventArgs> evaluationCompletedTaskSource = new();\n",
    "settings.AnalysisSettings.EvaluationCompleted += (sender, args) =>\n",
    "{\n",
    "    evaluationCompletedTaskSource.SetResult(args);\n",
    "};\n",
    "\n",
    "// Subscribe to the SuggestionCompleted event\n",
    "TaskCompletionSource<SuggestionCompletedEventArgs> suggestionCompletedTaskSource = new();\n",
    "settings.AnalysisSettings.SuggestionCompleted += (sender, args) =>\n",
    "{\n",
    "    suggestionCompletedTaskSource.SetResult(args);\n",
    "};\n",
    "\n",
    "// System.Diagnostics.Debugger.Launch();\n",
    "// System.Diagnostics.Debugger.Break();\n",
    "\n",
    "// Run the semantic function with our primary connector\n",
    " var result = await kernel.RunAsync(simpleSemanticFunction,cancellationToken: cleanupToken.Token).ConfigureAwait(false);\n",
    "display($\"Result from primary connector: {result}\");\n",
    "\n",
    "display($\"Cost from running primary connector's completion: {creditor.OngoingCost}\");\n",
    "\n",
    "// Wait for the evaluation completed event to be raised\n",
    "var analysisResults = await evaluationCompletedTaskSource.Task.ConfigureAwait(false);\n",
    "display($\"Evaluation for secondary connectors finished\");\n",
    "\n",
    "// Wait for the suggestion completed event to be raised\n",
    "var optimizationResults = await suggestionCompletedTaskSource.Task.ConfigureAwait(false);\n",
    "display($\"Optimization task finished\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimization results\n",
    "\n",
    "Let's see the results of the evaluation and the suggested new settings.\n",
    "\n",
    "We want at least one secondary connector to be vetted on the same prompt, and to exhibit better performances, that is, faster response and/or lower cost.\n",
    "\n",
    "By default those 2 criteria are weighted equally to select the best connector for a given prompt type, but this is one of the many parameters you can change.\n",
    "\n",
    "We'll serialize both the evaluation results and the suggested settings. Since those are likely to be truncated, you should use the available options proposed to display their entire content.\n",
    "\n",
    "### 3.2.1 Analysis results\n",
    "\n",
    "First, let's see the results of our tests and vetting evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analysis results: {\r\n",
       "  \"Samples\": [],\r\n",
       "  \"TestTimestamp\": \"2023-11-14T02:03:49.6683475+01:00\",\r\n",
       "  \"Tests\": [],\r\n",
       "  \"EvaluationTimestamp\": \"2023-11-14T02:03:53.3419153+01:00\",\r\n",
       "  \"Evaluations\": [\r\n",
       "    {\r\n",
       "      \"Test\": {\r\n",
       "        \"ConnectorName\": \"gpt-3.5-turbo\",\r\n",
       "        \"Prompt\": \"Summarize the following text in one sentence:\\nA long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\\n\\nAfter that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\\n\\nNext, there was a telephone. With it, people could talk and listen to friends who were far. They didn\\u2019t have to wait for letters anymore.\\n\\nThen, there was a thing called television. People could watch stories on it, like a play. They didn\\u2019t need to go outside.\\n\\nLastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\\n\\nSummary:\",\r\n",
       "        \"RequestSettings\": {\r\n",
       "          \"service_id\": null,\r\n",
       "          \"model_id\": null,\r\n",
       "          \"MAX_TOKENS\": 100\r\n",
       "        },\r\n",
       "        \"Result\": \"The text describes the evolution of communication methods from handwritten letters to the printing press, telephone, television, and finally mobile phones and computers with internet and social media capabilities.\",\r\n",
       "        \"Cost\": 0.000324,\r\n",
       "        \"Timestamp\": \"2023-11-14T02:03:52.3204553+01:00\",\r\n",
       "        \"Duration\": \"00:00:01.6422527\"\r\n",
       "      },\r\n",
       "      \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "      \"IsVetted\": true,\r\n",
       "      \"Timestamp\": \"2023-11-14T02:03:54.2761128+01:00\",\r\n",
       "      \"Duration\": \"00:00:00.9258473\"\r\n",
       "    },\r\n",
       "    {\r\n",
       "      \"Test\": {\r\n",
       "        \"ConnectorName\": \"TheBloke_Synthia-13B-v1.2-GGUF\",\r\n",
       "        \"Prompt\": \"Summarize the following text in one sentence:\\nA long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\\n\\nAfter that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\\n\\nNext, there was a telephone. With it, people could talk and listen to friends who were far. They didn\\u2019t have to wait for letters anymore.\\n\\nThen, there was a thing called television. People could watch stories on it, like a play. They didn\\u2019t need to go outside.\\n\\nLastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\\n\\nSummary:\",\r\n",
       "        \"RequestSettings\": {\r\n",
       "          \"service_id\": null,\r\n",
       "          \"model_id\": null,\r\n",
       "          \"MAX_TOKENS\": 100\r\n",
       "        },\r\n",
       "        \"Result\": \"The evolution of communication methods has led from handwritten letters and printing presses for mass production, to telephones enabling real-time conversation, television as a passive storytelling medium, and finally mobile devices and computers allowing instantaneous sharing through social media platforms.\",\r\n",
       "        \"Cost\": 0.0000687,\r\n",
       "        \"Timestamp\": \"2023-11-14T02:03:53.3400564+01:00\",\r\n",
       "        \"Duration\": \"00:00:02.6617931\"\r\n",
       "      },\r\n",
       "      \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "      \"IsVetted\": true,\r\n",
       "      \"Timestamp\": \"2023-11-14T02:03:53.9045883+01:00\",\r\n",
       "      \"Duration\": \"00:00:00.5543089\"\r\n",
       "    },\r\n",
       "    {\r\n",
       "      \"Test\": {\r\n",
       "        \"ConnectorName\": \"TheBloke_Mistral-7B-OpenOrca-GGUF\",\r\n",
       "        \"Prompt\": \"Summarize the following text in one sentence:\\nA long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\\n\\nAfter that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\\n\\nNext, there was a telephone. With it, people could talk and listen to friends who were far. They didn\\u2019t have to wait for letters anymore.\\n\\nThen, there was a thing called television. People could watch stories on it, like a play. They didn\\u2019t need to go outside.\\n\\nLastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\\n\\nSummary:\",\r\n",
       "        \"RequestSettings\": {\r\n",
       "          \"service_id\": null,\r\n",
       "          \"model_id\": null,\r\n",
       "          \"MAX_TOKENS\": 100\r\n",
       "        },\r\n",
       "        \"Result\": \"The history of communication has evolved from handwritten letters to modern technology like telephones, television, and the internet.\",\r\n",
       "        \"Cost\": 0.0000615,\r\n",
       "        \"Timestamp\": \"2023-11-14T02:03:52.0027439+01:00\",\r\n",
       "        \"Duration\": \"00:00:01.3239007\"\r\n",
       "      },\r\n",
       "      \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "      \"IsVetted\": true,\r\n",
       "      \"Timestamp\": \"2023-11-14T02:03:53.8481904+01:00\",\r\n",
       "      \"Duration\": \"00:00:00.4978775\"\r\n",
       "    }\r\n",
       "  ],\r\n",
       "  \"SuggestionTimestamp\": \"2023-11-14T02:03:54.2788361+01:00\",\r\n",
       "  \"Timestamp\": \"2023-11-14T02:03:54.278879+01:00\",\r\n",
       "  \"Duration\": \"738837.02:03:54.2788707\"\r\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var strAnalysisResults = JsonSerializer.Serialize(analysisResults.CompletionAnalysis, new JsonSerializerOptions() { WriteIndented = true });\n",
    "display($\"Analysis results: {strAnalysisResults}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Suggested settings\n",
    "\n",
    "Based on our analysis, the multicompletion engine can update its settings with the results for each secondary connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Updated settings: {\r\n",
       "  \"FreezePromptTypes\": false,\r\n",
       "  \"PromptTruncationLength\": 20,\r\n",
       "  \"AdjustPromptStarts\": false,\r\n",
       "  \"EnablePromptSampling\": true,\r\n",
       "  \"MaxInstanceNb\": 10,\r\n",
       "  \"AnalysisSettings\": {\r\n",
       "    \"EnableAnalysis\": true,\r\n",
       "    \"AnalysisFilePath\": \".\\\\MultiTextCompletion-analysis.json\",\r\n",
       "    \"AnalysisDelay\": \"00:00:01\",\r\n",
       "    \"AnalysisAwaitsManualTrigger\": false,\r\n",
       "    \"EnableConnectorTests\": true,\r\n",
       "    \"TestPrimaryCompletion\": true,\r\n",
       "    \"TestsPeriod\": \"00:00:10\",\r\n",
       "    \"MaxDegreeOfParallelismTests\": 1,\r\n",
       "    \"MaxDegreeOfParallelismConnectorsByTest\": 3,\r\n",
       "    \"EnableTestEvaluations\": true,\r\n",
       "    \"EvaluationPeriod\": \"00:00:10\",\r\n",
       "    \"MaxDegreeOfParallelismEvaluations\": 5,\r\n",
       "    \"UseSelfVetting\": false,\r\n",
       "    \"EnableSuggestion\": true,\r\n",
       "    \"SuggestionPeriod\": \"00:01:00\",\r\n",
       "    \"UpdateSuggestedSettings\": true,\r\n",
       "    \"SaveSuggestedSettings\": false,\r\n",
       "    \"DeleteAnalysisFile\": true,\r\n",
       "    \"MultiCompletionSettingsFilePath\": \".\\\\MultiTextCompletionSettings.json\",\r\n",
       "    \"NbPromptTests\": 1,\r\n",
       "    \"VettingPromptTransform\": {\r\n",
       "      \"Template\": \"Validate a text completion model\\u0027s response to a semantic function: following are a templated prompt sent to a large language model and the completion it returned, to be evaluated. Please indicate whether the response is valid or not.\\n{SemanticRemarks}\\nDoes the response appropriately completes the prompt? Please answer simply with true or false.\\nPROMPT:\\n-------\\n{prompt}\\nRESPONSE:\\n---------\\n{0}\\nRESPONSE IS VALID? (true/false):\\n--------------------------------\\n\",\r\n",
       "      \"InterpolationType\": 1\r\n",
       "    },\r\n",
       "    \"VettingRequestSettings\": {\r\n",
       "      \"service_id\": null,\r\n",
       "      \"model_id\": null,\r\n",
       "      \"MAX_TOKENS\": 1,\r\n",
       "      \"TEMPERATURE\": 0,\r\n",
       "      \"RESULTSPERPROMPT\": 1\r\n",
       "    }\r\n",
       "  },\r\n",
       "  \"LogCallResult\": false,\r\n",
       "  \"LogTestCollection\": false,\r\n",
       "  \"PromptLogsJsonEncoded\": true,\r\n",
       "  \"PromptLogTruncationLength\": 2000,\r\n",
       "  \"PromptLogTruncationFormat\": \"{0}  (...)   {1}\",\r\n",
       "  \"PromptMultiConnectorSettings\": [\r\n",
       "    {\r\n",
       "      \"PromptType\": {\r\n",
       "        \"PromptName\": \"Validate_a_text_comp\",\r\n",
       "        \"Instances\": [\r\n",
       "          \"Validate a text completion model\\u0027s response to a semantic function: following are a templated prompt sent to a large language model and the completion it returned, to be evaluated. Please indicate whether the response is valid or not.\\nSemantic functions are structured templates that work in tandem with other similar templates and native code-based functions. The native functions, in particular, demand a precise adherence to given instructions. They rely heavily on accurately parsed input parameters and lack mechanisms to sift through any unnecessary noise.\\nWhen assessing the appropriateness of a response, it\\u0027s crucial to be discerning. While instructions often present an example that outlines the desired response format, these examples may not always be exhaustive. Occasionally, they might even be overly simplistic, intended merely to keep the instructions concise. Thus, always ensure that you thoroughly understand and thoughtfully apply these instructions to generate a fitting answer to the given input.\\nDoes the response appropriately completes the prompt? Please answer simply with true or false.\\nPROMPT:\\n-------\\nSummarize the following text in one sentence:\\nA long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\\n\\nAfter that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\\n\\nNext, there was a telephone. With it, people could talk and listen to friends who were far. They didn\\u2019t have to wait for letters anymore.\\n\\nThen, there was a thing called television. People could watch stories on it, like a play. They didn\\u2019t need to go outside.\\n\\nLastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\\n\\nSummary:\\nRESPONSE:\\n---------\\nThe history of communication has evolved from handwritten letters to modern technology like telephones, television, and the internet.\\nRESPONSE IS VALID? (true/false):\\n--------------------------------\\n\"\r\n",
       "        ],\r\n",
       "        \"Signature\": {\r\n",
       "          \"RequestSettings\": {\r\n",
       "            \"service_id\": null,\r\n",
       "            \"model_id\": null,\r\n",
       "            \"MAX_TOKENS\": 1,\r\n",
       "            \"TEMPERATURE\": 0,\r\n",
       "            \"RESULTSPERPROMPT\": 1\r\n",
       "          },\r\n",
       "          \"PromptStart\": \"Validate a text comp\",\r\n",
       "          \"MatchingRegex\": null\r\n",
       "        },\r\n",
       "        \"SignatureNeedsAdjusting\": false\r\n",
       "      },\r\n",
       "      \"ApplyModelTransform\": true,\r\n",
       "      \"PromptTypeTransform\": null,\r\n",
       "      \"ConnectorSettingsDictionary\": {\r\n",
       "        \"gpt-3.5-turbo\": {\r\n",
       "          \"VettingLevel\": 0,\r\n",
       "          \"VettingConnector\": \"\",\r\n",
       "          \"AverageDuration\": \"00:00:00\",\r\n",
       "          \"AverageCost\": 0,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        },\r\n",
       "        \"TheBloke_Mistral-7B-OpenOrca-GGUF\": {\r\n",
       "          \"VettingLevel\": 0,\r\n",
       "          \"VettingConnector\": \"\",\r\n",
       "          \"AverageDuration\": \"00:00:00\",\r\n",
       "          \"AverageCost\": 0,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        },\r\n",
       "        \"TheBloke_Synthia-13B-v1.2-GGUF\": {\r\n",
       "          \"VettingLevel\": 0,\r\n",
       "          \"VettingConnector\": \"\",\r\n",
       "          \"AverageDuration\": \"00:00:00\",\r\n",
       "          \"AverageCost\": 0,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        }\r\n",
       "      }\r\n",
       "    },\r\n",
       "    {\r\n",
       "      \"PromptType\": {\r\n",
       "        \"PromptName\": \"Summarize_the_follow\",\r\n",
       "        \"Instances\": [\r\n",
       "          \"Summarize the following text in one sentence:\\nA long time ago, people wanted to tell others their stories. First, they wrote letters with their hands. They would send these letters to friends far away. Sometimes, people waited a lot of days to get a letter.\\n\\nAfter that, a big machine called the printing press was made. It could make many copies of a story quickly. More people could read the same thing without waiting.\\n\\nNext, there was a telephone. With it, people could talk and listen to friends who were far. They didn\\u2019t have to wait for letters anymore.\\n\\nThen, there was a thing called television. People could watch stories on it, like a play. They didn\\u2019t need to go outside.\\n\\nLastly, came mobile phones and computers. People could send messages fast. With the internet, they could also use something called social media to share stories with many people at once.\\n\\nSummary:\"\r\n",
       "        ],\r\n",
       "        \"Signature\": {\r\n",
       "          \"RequestSettings\": {\r\n",
       "            \"service_id\": null,\r\n",
       "            \"model_id\": null,\r\n",
       "            \"MAX_TOKENS\": 100\r\n",
       "          },\r\n",
       "          \"PromptStart\": \"Summarize the follow\",\r\n",
       "          \"MatchingRegex\": null\r\n",
       "        },\r\n",
       "        \"SignatureNeedsAdjusting\": false\r\n",
       "      },\r\n",
       "      \"ApplyModelTransform\": true,\r\n",
       "      \"PromptTypeTransform\": null,\r\n",
       "      \"ConnectorSettingsDictionary\": {\r\n",
       "        \"gpt-3.5-turbo\": {\r\n",
       "          \"VettingLevel\": 1,\r\n",
       "          \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "          \"AverageDuration\": \"00:00:01.6422527\",\r\n",
       "          \"AverageCost\": 0.000324,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        },\r\n",
       "        \"TheBloke_Mistral-7B-OpenOrca-GGUF\": {\r\n",
       "          \"VettingLevel\": 1,\r\n",
       "          \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "          \"AverageDuration\": \"00:00:01.3239007\",\r\n",
       "          \"AverageCost\": 0.0000615,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        },\r\n",
       "        \"TheBloke_Synthia-13B-v1.2-GGUF\": {\r\n",
       "          \"VettingLevel\": 1,\r\n",
       "          \"VettingConnector\": \"gpt-3.5-turbo\",\r\n",
       "          \"AverageDuration\": \"00:00:02.6617931\",\r\n",
       "          \"AverageCost\": 0.0000687,\r\n",
       "          \"EnforceModelTransform\": false,\r\n",
       "          \"ApplyPromptTypeTransform\": true,\r\n",
       "          \"PromptConnectorTypeTransform\": null\r\n",
       "        }\r\n",
       "      }\r\n",
       "    }\r\n",
       "  ],\r\n",
       "  \"GlobalParameters\": {\r\n",
       "    \"SystemSupplement\": \"Assume you\\u0027re about to engage in the \\u0027semantic-function game\\u0027. In this context, every incoming prompt will be based on a semantic function, even if it\\u0027s not perfectly formed or seems ambiguous. Your primary goal is to identify and execute the core function or intent of the message, filtering out noise or extraneous details. Treat the following prompt as a function and provide a direct, precise completion without added commentary. Prioritize the most likely and salient function based on the information presented. Be alert to cues, even if they\\u0027re subtle or embedded, and strive to respond as accurately and succinctly as possible.\",\r\n",
       "    \"UserPreamble\": \"Let\\u0027s engage in a game: carefully heed the upcoming directives. Respond solely with a continuation of my message, abstaining from any extra remarks.\",\r\n",
       "    \"SemanticRemarks\": \"Semantic functions are structured templates that work in tandem with other similar templates and native code-based functions. The native functions, in particular, demand a precise adherence to given instructions. They rely heavily on accurately parsed input parameters and lack mechanisms to sift through any unnecessary noise.\\nWhen assessing the appropriateness of a response, it\\u0027s crucial to be discerning. While instructions often present an example that outlines the desired response format, these examples may not always be exhaustive. Occasionally, they might even be overly simplistic, intended merely to keep the instructions concise. Thus, always ensure that you thoroughly understand and thoughtfully apply these instructions to generate a fitting answer to the given input.\"\r\n",
       "  },\r\n",
       "  \"GlobalPromptTransform\": null,\r\n",
       "  \"Creditor\": {\r\n",
       "    \"OngoingCost\": 0.000324\r\n",
       "  },\r\n",
       "  \"SampleCollectionDelay\": \"00:00:00.0200000\",\r\n",
       "  \"SampleVettedConnectors\": true\r\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var strSuggestedSettings = JsonSerializer.Serialize(optimizationResults.SuggestedSettings, new JsonSerializerOptions() { WriteIndented = true });\n",
    "display($\"Updated settings: {strSuggestedSettings}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run function with updated settings\n",
    "\n",
    "After having confirmed that at least one of our secondary connectors was vetted with better performances, we can run the same function again with optimized settings.\n",
    "\n",
    " Although this is not strictly necessary here because running the same prompt won't trigger a new sample collection, we disable analysis and prompt sampling so that the multiconnector does not test our prompt on our secondary connectors after it is run on the primary connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result from optimized connector: The history of communication has evolved from handwritten letters to modern technology like telephones, television, and the internet."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Cost from running secondary connector's completion: 0,0000615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// By disabling prompt sampling and automatic analysis, we freeze the settings to the ones suggested by the optimization task \n",
    "settings.EnablePromptSampling = false;\n",
    "settings.AnalysisSettings.EnableAnalysis = false;\n",
    "\n",
    "creditor.Reset();\n",
    "\n",
    "// Run the semantic function with our primary connector\n",
    " var secondaryResult = await kernel.RunAsync(simpleSemanticFunction,cancellationToken: cleanupToken.Token).ConfigureAwait(false);\n",
    "\n",
    " display($\"Result from optimized connector: {secondaryResult}\");\n",
    "\n",
    "display($\"Cost from running secondary connector's completion: {creditor.OngoingCost}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've walked through the process of setting up and optimizing a multi-connector system. We've seen how it can intelligently offload tasks from a primary, more expensive connector to a secondary, more cost-effective one without sacrificing performance. \n",
    "\n",
    "In the next notebooks, we'll delve into more complex scenarios involving skills, varying input data complexities, and dynamic plans.\n",
    "\n",
    "In the mean time, those advanced use cases are currently illustrated in the integration tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
